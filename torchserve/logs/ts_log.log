2025-04-21T00:45:54,144 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, 没有那个文件或目录
2025-04-21T00:45:54,144 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program "xpu-smi": error=2, 没有那个文件或目录
2025-04-21T00:45:54,148 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-04-21T00:45:54,148 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2025-04-21T00:45:54,157 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-04-21T00:45:54,157 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2025-04-21T00:45:54,211 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-04-21T00:45:54,211 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
2025-04-21T00:45:54,287 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ucas/.local/lib/python3.10/site-packages
Current directory: /home/ucas/FireLLM/torchserve
Temp directory: /tmp
Metrics config path: /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 16068 M
Python executable: /usr/bin/python3
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/ucas/FireLLM/torchserve/model_store
Initial Models: yolov8_fire_smoke=vlm_model.mar
Log dir: /home/ucas/FireLLM/torchserve/logs
Metrics dir: /home/ucas/FireLLM/torchserve/logs
Netty threads: 8
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ucas/FireLLM/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-04-21T00:45:54,287 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.12.0
TS Home: /home/ucas/.local/lib/python3.10/site-packages
Current directory: /home/ucas/FireLLM/torchserve
Temp directory: /tmp
Metrics config path: /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 16
Max heap size: 16068 M
Python executable: /usr/bin/python3
Config file: config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/ucas/FireLLM/torchserve/model_store
Initial Models: yolov8_fire_smoke=vlm_model.mar
Log dir: /home/ucas/FireLLM/torchserve/logs
Metrics dir: /home/ucas/FireLLM/torchserve/logs
Netty threads: 8
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: /home/ucas/FireLLM/torchserve/model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
Model API enabled: false
2025-04-21T00:45:54,295 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-04-21T00:45:54,295 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2025-04-21T00:45:54,314 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: vlm_model.mar
2025-04-21T00:45:54,314 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: vlm_model.mar
2025-04-21T00:45:54,505 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model yolov8_fire_smoke
2025-04-21T00:45:54,505 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model yolov8_fire_smoke
2025-04-21T00:45:54,506 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model yolov8_fire_smoke
2025-04-21T00:45:54,506 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model yolov8_fire_smoke
2025-04-21T00:45:54,506 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model yolov8_fire_smoke loaded.
2025-04-21T00:45:54,506 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model yolov8_fire_smoke loaded.
2025-04-21T00:45:54,506 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: yolov8_fire_smoke, count: 1
2025-04-21T00:45:54,506 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: yolov8_fire_smoke, count: 1
2025-04-21T00:45:54,515 [DEBUG] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ucas/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-04-21T00:45:54,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-04-21T00:45:54,515 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2025-04-21T00:45:54,515 [DEBUG] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /home/ucas/.local/lib/python3.10/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml]
2025-04-21T00:45:54,582 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-04-21T00:45:54,582 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2025-04-21T00:45:54,583 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-04-21T00:45:54,583 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2025-04-21T00:45:54,586 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2025-04-21T00:45:54,586 [INFO ] epollEventLoopGroup-2-1 org.pytorch.serve.ModelServer - Inference model server stopped.
2025-04-21T00:45:55,907 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=715624
2025-04-21T00:45:55,908 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2025-04-21T00:45:55,915 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - Successfully loaded /home/ucas/.local/lib/python3.10/site-packages/ts/configs/metrics.yaml.
2025-04-21T00:45:55,916 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - [PID]715624
2025-04-21T00:45:55,916 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - Torch worker started.
2025-04-21T00:45:55,916 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - Python runtime: 3.10.12
2025-04-21T00:45:55,916 [DEBUG] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-yolov8_fire_smoke_1.0 State change null -> WORKER_STARTED
2025-04-21T00:45:55,916 [DEBUG] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-yolov8_fire_smoke_1.0 State change null -> WORKER_STARTED
2025-04-21T00:45:55,921 [INFO ] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-04-21T00:45:55,921 [INFO ] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2025-04-21T00:45:55,928 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2025-04-21T00:45:55,931 [DEBUG] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1745167555931
2025-04-21T00:45:55,931 [DEBUG] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1745167555931
2025-04-21T00:45:55,933 [INFO ] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1745167555933
2025-04-21T00:45:55,933 [INFO ] W-9000-yolov8_fire_smoke_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1745167555933
2025-04-21T00:45:55,961 [INFO ] W-9000-yolov8_fire_smoke_1.0-stdout MODEL_LOG - model_name: yolov8_fire_smoke, batchSize: 1
2025-04-21T00:45:56,604 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
2025-04-21T00:45:56,604 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.
